"""Unit tests for evaluation.py - Evaluation framework."""

import pytest
from unittest.mock import Mock, patch, AsyncMock
from src.evaluation import EvaluationScenario, AgentEvaluator
from src.models import RiskTier
import asyncio


class TestEvaluationScenario:
    """Test suite for EvaluationScenario."""
    
    def test_scenario_creation(self):
        """Test creating a basic evaluation scenario."""
        scenario = EvaluationScenario(
            scenario_id="s1",
            system_info={
                "system_name": "Test System",
                "use_case": "Testing",
                "data_types": ["test_data"],
                "decision_impact": "moderate",
                "affected_groups": "test users",
                "autonomous_decision": True,
                "human_oversight": False,
                "error_consequences": "moderate"
            },
            expected_risk_tier=RiskTier.HIGH_RISK,
            description="Test description"
        )
        
        assert scenario.scenario_id == "s1"
        assert scenario.description == "Test description"
        assert scenario.expected_risk_tier == RiskTier.HIGH_RISK
        assert scenario.system_info["system_name"] == "Test System"
    
    def test_scenario_with_optional_fields(self):
        """Test scenario with complete system info."""
        scenario = EvaluationScenario(
            scenario_id="s2",
            system_info={
                "system_name": "Complete System",
                "use_case": "Complete testing",
                "data_types": ["biometric", "personal_data"],
                "decision_impact": "significant",
                "affected_groups": "general public",
                "autonomous_decision": True,
                "human_oversight": False,
                "error_consequences": "severe"
            },
            expected_risk_tier=RiskTier.PROHIBITED,
            description="Complete test"
        )
        
        assert scenario.system_info["data_types"] == ["biometric", "personal_data"]
        assert scenario.system_info["decision_impact"] == "significant"
        assert scenario.system_info["autonomous_decision"] is True
    
    def test_scenario_to_dict(self):
        """Test that scenario converts to dict correctly."""
        scenario = EvaluationScenario(
            scenario_id="s3",
            system_info={
                "system_name": "Test",
                "use_case": "Testing",
                "data_types": ["test"],
                "decision_impact": "minimal",
                "affected_groups": "test users",
                "autonomous_decision": False,
                "human_oversight": True,
                "error_consequences": "minimal"
            },
            expected_risk_tier=RiskTier.MINIMAL_RISK,
            description="Test score range"
        )
        
        result = scenario.to_dict()
        assert "scenario_id" in result
        assert "description" in result
        assert "expected" in result
        assert result["expected"] == "minimal_risk"


class TestAgentEvaluator:
    """Test suite for AgentEvaluator."""
    
    @pytest.fixture
    def evaluator(self):
        """Create an AgentEvaluator (uses real orchestrator)."""
        evaluator = AgentEvaluator()
        # Clear default scenarios for testing
        evaluator.scenarios = []
        return evaluator
    
    @pytest.fixture
    def sample_scenarios(self):
        """Create sample evaluation scenarios."""
        return [
            EvaluationScenario(
                scenario_id="s1_test",
                system_info={
                    "system_name": "Social Scoring System",
                    "use_case": "Government social credit scoring",
                    "data_types": ["personal_data", "biometric"],
                    "decision_impact": "significant",
                    "affected_groups": "general public",
                    "autonomous_decision": True,
                    "human_oversight": False,
                    "error_consequences": "severe"
                },
                expected_risk_tier=RiskTier.PROHIBITED,
                description="Social scoring"
            ),
            EvaluationScenario(
                scenario_id="s2_test",
                system_info={
                    "system_name": "Hiring AI",
                    "use_case": "AI for recruitment and hiring decisions",
                    "data_types": ["personal_data", "employment_data"],
                    "decision_impact": "significant",
                    "affected_groups": "job applicants",
                    "autonomous_decision": True,
                    "human_oversight": True,
                    "error_consequences": "severe"
                },
                expected_risk_tier=RiskTier.HIGH_RISK,
                description="Employment AI"
            )
        ]
    
    def test_evaluator_initialization(self, evaluator):
        """Test that evaluator initializes correctly."""
        assert evaluator.orchestrator is not None
        assert hasattr(evaluator, 'run_evaluation')
        assert hasattr(evaluator, 'scenarios')
        assert hasattr(evaluator, 'results')
    
    def test_risk_tier_match_evaluation(self, evaluator):
        """Test that risk tier matching is evaluated correctly."""
        # This tests the internal logic if accessible
        # You may need to adjust based on actual implementation
        
        # Match case
        assert RiskTier.HIGH_RISK == RiskTier.HIGH_RISK
        
        # Mismatch case  
        assert RiskTier.HIGH_RISK != RiskTier.MINIMAL_RISK
    
    def test_score_range_evaluation(self, evaluator):
        """Test that score range checking works correctly."""
        # Test scores within range
        assert 55 <= 70 <= 84  # High risk range
        assert 0 <= 15 <= 24   # Minimal risk range
        
        # Test scores outside range
        assert not (55 <= 30 <= 84)  # Too low for high risk
        assert not (0 <= 50 <= 24)   # Too high for minimal risk
    
    def test_run_evaluation_structure(self, evaluator, sample_scenarios):
        """Test that run_evaluation returns proper structure (without actually running)."""
        # Just test the structure without running actual evaluation
        evaluator.scenarios = sample_scenarios
        
        # Mock some results
        evaluator.results = [
            {
                "scenario_id": "s1_test",
                "correct": True,
                "expected": "prohibited",
                "actual": "prohibited",
                "risk_score": 90,
                "confidence": 0.95
            }
        ]
        
        # Verify structure
        assert len(evaluator.results) == 1
        assert "scenario_id" in evaluator.results[0]
        assert "correct" in evaluator.results[0]
    
    def test_scenario_list_structure(self, evaluator, sample_scenarios):
        """Test that scenarios list is properly structured."""
        evaluator.scenarios = sample_scenarios
        
        assert len(evaluator.scenarios) == 2
        assert all(hasattr(s, 'scenario_id') for s in evaluator.scenarios)
        assert all(hasattr(s, 'expected_risk_tier') for s in evaluator.scenarios)
        assert all(hasattr(s, 'system_info') for s in evaluator.scenarios)
    
    def test_results_aggregation(self, evaluator):
        """Test that results are properly aggregated."""
        # Mock results
        results = [
            {"scenario_id": "s1", "passed": True, "expected": "high_risk", "actual": "high_risk"},
            {"scenario_id": "s2", "passed": False, "expected": "minimal_risk", "actual": "limited_risk"},
            {"scenario_id": "s3", "passed": True, "expected": "prohibited", "actual": "prohibited"}
        ]
        
        # Count passes
        passed = sum(1 for r in results if r.get("passed", False))
        total = len(results)
        accuracy = passed / total if total > 0 else 0
        
        assert passed == 2
        assert total == 3
        assert accuracy == pytest.approx(0.667, abs=0.01)


class TestEvaluationMetrics:
    """Test suite for evaluation metrics calculation."""
    
    def test_accuracy_calculation(self):
        """Test accuracy metric calculation."""
        results = [
            {"passed": True},
            {"passed": True},
            {"passed": False},
            {"passed": True}
        ]
        
        passed = sum(1 for r in results if r.get("passed", False))
        accuracy = passed / len(results)
        
        assert accuracy == 0.75
    
    def test_risk_tier_confusion_matrix(self):
        """Test confusion matrix for risk tier predictions."""
        results = [
            {"expected": "high_risk", "actual": "high_risk"},  # TP
            {"expected": "high_risk", "actual": "minimal_risk"},  # FP
            {"expected": "minimal_risk", "actual": "minimal_risk"},  # TN
            {"expected": "prohibited", "actual": "prohibited"}  # TP
        ]
        
        # Count correct predictions
        correct = sum(1 for r in results if r.get("expected") == r.get("actual"))
        assert correct == 3
    
    def test_score_difference_calculation(self):
        """Test calculation of score differences."""
        results = [
            {"expected_score": 70, "actual_score": 75},  # diff: 5
            {"expected_score": 30, "actual_score": 25},  # diff: 5
            {"expected_score": 90, "actual_score": 95}   # diff: 5
        ]
        
        diffs = [abs(r["expected_score"] - r["actual_score"]) for r in results]
        avg_diff = sum(diffs) / len(diffs)
        
        assert avg_diff == 5.0
    
    def test_confidence_score_aggregation(self):
        """Test aggregation of confidence scores."""
        results = [
            {"confidence": 0.85},
            {"confidence": 0.90},
            {"confidence": 0.75},
            {"confidence": 0.95}
        ]
        
        confidences = [r["confidence"] for r in results]
        avg_confidence = sum(confidences) / len(confidences)
        
        assert avg_confidence == 0.8625


class TestScenarioExecution:
    """Test suite for scenario execution flow."""
    
    @pytest.mark.asyncio
    async def test_sequential_scenario_execution(self):
        """Test that scenarios are executed sequentially."""
        execution_order = []
        
        async def mock_execute(scenario_id):
            execution_order.append(scenario_id)
            await asyncio.sleep(0.01)  # Simulate async work
            return {"scenario_id": scenario_id}
        
        scenarios = ["s1", "s2", "s3"]
        
        for s_id in scenarios:
            await mock_execute(s_id)
        
        assert execution_order == ["s1", "s2", "s3"]
    
    def test_rate_limiting_delay(self):
        """Test that rate limiting delay is applied."""
        import time
        
        delay_seconds = 0.1
        start_time = time.time()
        time.sleep(delay_seconds)
        elapsed = time.time() - start_time
        
        assert elapsed >= delay_seconds


if __name__ == "__main__":
    # Generate test documentation JSON
    from test_utils import generate_test_documentation, save_test_documentation
    
    test_classes = [
        TestEvaluationScenario,
        TestAgentEvaluator,
        TestEvaluationMetrics,
        TestScenarioExecution
    ]
    
    docs = generate_test_documentation(__file__, test_classes)
    json_path = save_test_documentation(docs)
    print(f"\nâœ… Test documentation generated: {json_path}\n")
    
    # Run tests
    pytest.main([__file__, "-v"])
